{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State,agent_player):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        if agent_player == 'x':\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    elif win(pos.o):\n",
    "        if agent_player == 'o':\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning Model free**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, alpha, epsilon, dis_factor):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.dis_factor = dis_factor\n",
    "\n",
    "    def get_Q(self, state, action):\n",
    "        state_key = (tuple(state.x), tuple(state.o))\n",
    "        return self.Q[(state_key, action)]\n",
    "\n",
    "\n",
    "    def choose_action(self, state, available):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available)\n",
    "        else:\n",
    "            Q_vals= [self.get_Q(state, action) for action in available]\n",
    "            max_Q = max(Q_vals)\n",
    "            best_moves = [i for i in range(len(available)) if Q_vals[i] == max_Q]\n",
    "            index = random.choice(best_moves)\n",
    "            return available[index]\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, available):\n",
    "        state_key = (tuple(state.x), tuple(state.o))\n",
    "        next_Q_vals = [self.get_Q(next_state, next_action) for next_action in available]\n",
    "        max_next_Q = max(next_Q_vals,default=0.0)\n",
    "        self.Q[(state_key, action)] = (1 - self.alpha) * self.Q[(state_key, action)] + self.alpha * (reward + self.dis_factor * max_next_Q)\n",
    "    \n",
    "\n",
    "def train(num_episodes, alpha, epsilon, disc_factor, agent_player):\n",
    "    agent = QLearning(alpha, epsilon, disc_factor)\n",
    "    for i in range(num_episodes):\n",
    "        state = State(set(), set())\n",
    "        available = list(range(1, 9 + 1))\n",
    "        player_turn = 'x' #we assumed that x is starting always firstly in our games function\n",
    "\n",
    "        while available and not win(state):\n",
    "            if player_turn == agent_player:\n",
    "                action = agent.choose_action(state, available)\n",
    "            else:\n",
    "                #if the current turn is of the adversary it is doing a random move\n",
    "                action = choice(available)\n",
    "\n",
    "            previous_state = deepcopy(state)\n",
    "\n",
    "            if player_turn == 'x':\n",
    "                state.x.add(action)\n",
    "            else:\n",
    "                state.o.add(action)\n",
    "\n",
    "            available.remove(action)\n",
    "\n",
    "            reward = state_value(state, agent_player)\n",
    "            agent.update_Q(previous_state, action, reward, state, available)\n",
    "\n",
    "            # Switching the player\n",
    "            player_turn = 'o' if player_turn == 'x' else 'x'\n",
    "\n",
    "        #Switching the agent to train the model in both the sides\n",
    "        agent_player = 'x' if agent_player == 'o' else 'o'\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(agent,agent_player): #Game which is considering both when the agent starts first or second thanks to the variable agent_player\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = list(range(1, 9+1))\n",
    "    if agent_player == 'x':\n",
    "        while available:\n",
    "            x = agent.choose_action(state,available)\n",
    "            state.x.add(x)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = choice(list(available))\n",
    "            state.o.add(o)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(o)\n",
    "            if win(state.o) or not available:\n",
    "                break\n",
    "    elif agent_player == 'o':\n",
    "        while available:\n",
    "            x = choice(list(available))\n",
    "            state.x.add(x)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = agent.choose_action(state,available)\n",
    "            state.o.add(o)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(o)\n",
    "            if win(state.o) or not available:\n",
    "                break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 1000\n",
    "NUM_EPISODES = 10000\n",
    "agent_player = 'o' #variable tunable to choose the agent player, we assumed that 'x' is always the first player while 'o' is always the second one\n",
    "#List of parameters to find a best value\n",
    "epsilon_values = [0.1,0.3,0.5]\n",
    "alpha_values = [0.2,0.5,0.9]\n",
    "disc_factor_values = [0.3, 0.7,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_parameters(epsilon_values, alpha_values, disc_factor_values,agent_player,num_episodes):\n",
    "    best_agent = None\n",
    "    best_percentage_win_agent = 0\n",
    "\n",
    "    for epsilon in epsilon_values:\n",
    "        for alpha in alpha_values:\n",
    "            for disc_factor in disc_factor_values:\n",
    "                agent = train(num_episodes=num_episodes, alpha=alpha, epsilon=epsilon, disc_factor=disc_factor,agent_player=agent_player)\n",
    "\n",
    "                num_win_agent=0\n",
    "                num_win_random_player=0\n",
    "                num_draw=0\n",
    "                for i in range(NUM_GAMES):\n",
    "                    trajectory=game(agent,agent_player)\n",
    "                    val_finished_game=state_value(trajectory[-1],agent_player)\n",
    "                    if val_finished_game == 1:\n",
    "                        num_win_agent+=1\n",
    "                    elif val_finished_game == -1:\n",
    "                        num_win_random_player+=1\n",
    "                    else:\n",
    "                        num_draw+=1\n",
    "\n",
    "                total_games = num_win_agent + num_win_random_player + num_draw\n",
    "                percentage_win_agent = (num_win_agent / total_games) * 100\n",
    "\n",
    "                if percentage_win_agent > best_percentage_win_agent:\n",
    "                    best_percentage_win_agent = percentage_win_agent\n",
    "                    best_agent = agent\n",
    "\n",
    "    return best_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent = search_best_parameters(epsilon_values, alpha_values, disc_factor_values,agent_player,NUM_EPISODES)\n",
    "\n",
    "num_win_agent=0\n",
    "num_win_random_player=0\n",
    "num_draw=0\n",
    "for i in range(NUM_GAMES):\n",
    "    trajectory=game(best_agent,agent_player)\n",
    "    val_finished_game=state_value(trajectory[-1],agent_player)\n",
    "    if val_finished_game == 1:\n",
    "        num_win_agent+=1\n",
    "    elif val_finished_game == -1:\n",
    "        num_win_random_player+=1\n",
    "    else:\n",
    "        num_draw+=1\n",
    "\n",
    "total_games = num_win_random_player + num_win_agent + num_draw\n",
    "\n",
    "percentage_win_agent = (num_win_agent / total_games) * 100\n",
    "percentage_win_random_player = (num_win_random_player / total_games) * 100\n",
    "percentage_draw = (num_draw / total_games) * 100\n",
    "percentage_win_agent_respect_random_player = (num_win_agent/(num_win_agent+num_win_random_player))*100\n",
    "\n",
    "print(\"Best parameters found\")\n",
    "print(f\"Epsilon: {best_agent.epsilon}\")\n",
    "print(f\"Alpha: {best_agent.alpha}\")\n",
    "print(f\"Discount Factor: {best_agent.dis_factor}\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Percentage wins of the agent: {percentage_win_agent}%\")\n",
    "print(f\"Percentage wins of the random player: {percentage_win_random_player}%\")\n",
    "print(f\"Percentage of draws: {percentage_draw}%\")\n",
    "print(f\"Percentage wins of the agent with respect to the random player: {percentage_win_agent_respect_random_player}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
